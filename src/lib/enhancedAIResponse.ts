import { Message } from '../types/message';
import { AICharacter } from '../types/character';
import { contextManager, processContextForAI, ContextProcessingResult } from './contextManager';
import { modelDefaults } from '../data/modelDefaults';

// å¢å¼ºAIå“åº”é…ç½®
export interface EnhancedAIResponseConfig {
  // ä¸Šä¸‹æ–‡ç®¡ç†é…ç½®
  enableContextPruning: boolean;
  maxContextTokens: number;
  enablePersonalization: boolean;
  
  // è°ƒè¯•é…ç½®
  debugMode: boolean;
  logContextInfo: boolean;
  
  // å›é€€é…ç½®
  fallbackToOriginal: boolean;
  fallbackThreshold: number;
  
  // æ€§èƒ½é…ç½®
  enableCaching: boolean;
  timeout: number;
}

// å¢å¼ºAIå“åº”ç»“æœ
export interface EnhancedAIResponseResult {
  success: boolean;
  response: string | null;
  
  // ä¸Šä¸‹æ–‡å¤„ç†ä¿¡æ¯
  contextInfo?: {
    originalMessageCount: number;
    processedMessageCount: number;
    tokenReduction: number;
    processingTime: number;
    strategy: string;
    usedPersonalization: boolean;
  };
  
  // Tokenä½¿ç”¨ä¿¡æ¯
  tokenUsage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  
  // æ€§èƒ½ä¿¡æ¯
  performanceInfo?: {
    totalTime: number;
    contextProcessingTime: number;
    aiResponseTime: number;
    cacheHit: boolean;
  };
  
  // é”™è¯¯ä¿¡æ¯
  error?: string;
  fallbackUsed?: boolean;
}

// é»˜è®¤é…ç½®
const defaultConfig: EnhancedAIResponseConfig = {
  enableContextPruning: true,
  maxContextTokens: 4000,
  enablePersonalization: true,
  debugMode: false,
  logContextInfo: true,
  fallbackToOriginal: true,
  fallbackThreshold: 0.5,
  enableCaching: true,
  timeout: 30000
};

/**
 * å¢å¼ºç‰ˆAIå“åº”å‡½æ•° - é›†æˆåŠ¨æ€ä¸Šä¸‹æ–‡è£å‰ªç³»ç»Ÿ
 */
export async function fetchEnhancedAIResponse(
  character: AICharacter,
  messages: Message[],
  updateTokenUsageFn?: (characterId: string, characterName: string, inputTokens: number, outputTokens: number, type: string) => void,
  estimateTokensFn?: (text: string) => number,
  config: Partial<EnhancedAIResponseConfig> = {}
): Promise<EnhancedAIResponseResult> {
  const startTime = Date.now();
  const finalConfig = { ...defaultConfig, ...config };
  
  try {
    console.log(`ğŸš€ å¼€å§‹å¢å¼ºAIå“åº” - ${character.name}`);
    
    // ç¬¬ä¸€æ­¥ï¼šä¸Šä¸‹æ–‡å¤„ç†
    let processedMessages = messages;
    let contextResult: ContextProcessingResult | null = null;
    
    if (finalConfig.enableContextPruning && messages.length > 0) {
      const contextStartTime = Date.now();
      
      try {
        contextResult = await processContextForAI(
          messages,
          character,
          undefined, // å½“å‰è¯é¢˜å¯ä»¥ä»æ¶ˆæ¯ä¸­æ¨æ–­
          {
            maxTokens: finalConfig.maxContextTokens,
            enablePersonalization: finalConfig.enablePersonalization,
            debugMode: finalConfig.debugMode
          }
        );
        
        if (contextResult.success) {
          processedMessages = contextResult.prunedMessages;
          
          if (finalConfig.logContextInfo) {
            console.log(`ğŸ§  ä¸Šä¸‹æ–‡è£å‰ªå®Œæˆ - ${character.name}:`, {
              åŸå§‹æ¶ˆæ¯æ•°: contextResult.originalMessageCount,
              å¤„ç†åæ¶ˆæ¯æ•°: contextResult.finalMessageCount,
              Tokenå‡å°‘: `${contextResult.tokenReduction.toFixed(1)}%`,
              ç­–ç•¥: contextResult.strategy,
              å¤„ç†æ—¶é—´: `${contextResult.processingTime}ms`
            });
          }
        } else {
          console.warn(`âš ï¸ ä¸Šä¸‹æ–‡å¤„ç†å¤±è´¥ - ${character.name}:`, contextResult.error);
          if (finalConfig.fallbackToOriginal) {
            processedMessages = messages;
          }
        }
      } catch (error) {
        console.error(`âŒ ä¸Šä¸‹æ–‡å¤„ç†å¼‚å¸¸ - ${character.name}:`, error);
        if (finalConfig.fallbackToOriginal) {
          processedMessages = messages;
        } else {
          throw error;
        }
      }
    }
    
    // ç¬¬äºŒæ­¥ï¼šç”ŸæˆAIå“åº”
    const aiResponseStartTime = Date.now();
    const aiResult = await generateAIResponse(character, processedMessages, finalConfig);
    const aiResponseTime = Date.now() - aiResponseStartTime;
    
    if (!aiResult.success) {
      throw new Error(aiResult.error || 'AIå“åº”ç”Ÿæˆå¤±è´¥');
    }
    
    // ç¬¬ä¸‰æ­¥ï¼šè®°å½•Tokenä½¿ç”¨
    if (aiResult.tokenUsage && updateTokenUsageFn) {
      updateTokenUsageFn(
        character.id,
        character.name,
        aiResult.tokenUsage.promptTokens,
        aiResult.tokenUsage.completionTokens,
        'enhanced-character'
      );
    }
    
    const totalTime = Date.now() - startTime;
    
    // æ„å»ºç»“æœ
    const result: EnhancedAIResponseResult = {
      success: true,
      response: aiResult.response,
      contextInfo: contextResult ? {
        originalMessageCount: contextResult.originalMessageCount,
        processedMessageCount: contextResult.finalMessageCount,
        tokenReduction: contextResult.tokenReduction,
        processingTime: contextResult.processingTime,
        strategy: contextResult.strategy,
        usedPersonalization: contextResult.metadata.usedPersonalization
      } : undefined,
      tokenUsage: aiResult.tokenUsage,
      performanceInfo: {
        totalTime,
        contextProcessingTime: contextResult?.processingTime || 0,
        aiResponseTime,
        cacheHit: false // TODO: å®ç°ç¼“å­˜æ£€æµ‹
      }
    };
    
    if (finalConfig.debugMode) {
      console.log(`âœ… å¢å¼ºAIå“åº”å®Œæˆ - ${character.name}:`, {
        æ€»è€—æ—¶: `${totalTime}ms`,
        ä¸Šä¸‹æ–‡å¤„ç†: `${result.performanceInfo?.contextProcessingTime}ms`,
        AIå“åº”: `${aiResponseTime}ms`,
        ä½¿ç”¨ä¸ªæ€§åŒ–: result.contextInfo?.usedPersonalization,
        Tokenå‡å°‘: result.contextInfo ? `${result.contextInfo.tokenReduction.toFixed(1)}%` : 'æœªå¯ç”¨'
      });
    }
    
    return result;
    
  } catch (error) {
    console.error(`âŒ å¢å¼ºAIå“åº”å¤±è´¥ - ${character.name}:`, error);
    
    // å›é€€åˆ°åŸå§‹å®ç°
    if (finalConfig.fallbackToOriginal) {
      console.log(`ğŸ”„ å›é€€åˆ°åŸå§‹AIå“åº” - ${character.name}`);
      
      try {
        const fallbackResult = await generateOriginalAIResponse(character, messages, estimateTokensFn, updateTokenUsageFn);
        
        return {
          success: true,
          response: fallbackResult.response,
          performanceInfo: {
            totalTime: Date.now() - startTime,
            contextProcessingTime: 0,
            aiResponseTime: fallbackResult.responseTime,
            cacheHit: false
          },
          tokenUsage: fallbackResult.tokenUsage,
          fallbackUsed: true,
          error: `å¢å¼ºå“åº”å¤±è´¥ï¼Œä½¿ç”¨å›é€€: ${error}`
        };
      } catch (fallbackError) {
        return {
          success: false,
          response: null,
          error: `å¢å¼ºå“åº”å’Œå›é€€éƒ½å¤±è´¥: ${error}, ${fallbackError}`,
          performanceInfo: {
            totalTime: Date.now() - startTime,
            contextProcessingTime: 0,
            aiResponseTime: 0,
            cacheHit: false
          }
        };
      }
    } else {
      return {
        success: false,
        response: null,
        error: error instanceof Error ? error.message : String(error),
        performanceInfo: {
          totalTime: Date.now() - startTime,
          contextProcessingTime: 0,
          aiResponseTime: 0,
          cacheHit: false
        }
      };
    }
  }
}

/**
 * ç”ŸæˆAIå“åº”
 */
async function generateAIResponse(
  character: AICharacter,
  messages: Message[],
  config: EnhancedAIResponseConfig
): Promise<{
  success: boolean;
  response: string | null;
  tokenUsage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  error?: string;
}> {
  const modelConfig = {
    baseUrl: character.modelConfig?.baseUrl || modelDefaults.baseUrl,
    apiKey: character.modelConfig?.apiKey || modelDefaults.apiKey,
    modelName: character.modelConfig?.modelName || modelDefaults.modelName,
    prompt: character.modelConfig?.prompt || '',
    temperature: character.modelConfig?.temperature ?? 0.7,
    maxTokens: character.modelConfig?.maxTokens ?? 2048,
    topP: character.modelConfig?.topP ?? 1.0,
    frequencyPenalty: character.modelConfig?.frequencyPenalty ?? 0.0,
    presencePenalty: character.modelConfig?.presencePenalty ?? 0.0,
  };
  
  if (!modelConfig.baseUrl || !modelConfig.apiKey || !modelConfig.modelName) {
    throw new Error('æ¨¡å‹é…ç½®ä¸å®Œæ•´');
  }
  
  // æ„å»ºè¯·æ±‚æ¶ˆæ¯
  const requestMessages = [
    ...(modelConfig.prompt ? [{ role: 'system', content: modelConfig.prompt }] : []),
    ...messages.map(m => ({
      role: m.isPlayer ? 'user' : 'assistant',
      content: m.text || m.content || ''
    }))
  ];
  
  const requestBody = {
    model: modelConfig.modelName,
    messages: requestMessages,
    temperature: modelConfig.temperature,
    max_tokens: modelConfig.maxTokens,
    top_p: modelConfig.topP,
    frequency_penalty: modelConfig.frequencyPenalty,
    presence_penalty: modelConfig.presencePenalty,
  };
  
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), config.timeout);
    
    const response = await fetch(modelConfig.baseUrl + '/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${modelConfig.apiKey}`,
      },
      body: JSON.stringify(requestBody),
      signal: controller.signal
    });
    
    clearTimeout(timeoutId);
    
    if (!response.ok) {
      throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status} ${response.statusText}`);
    }
    
    const data = await response.json();
    const responseContent = data.choices?.[0]?.message?.content;
    
    if (!responseContent) {
      throw new Error('APIè¿”å›å†…å®¹ä¸ºç©º');
    }
    
    return {
      success: true,
      response: responseContent,
      tokenUsage: data.usage ? {
        promptTokens: data.usage.prompt_tokens || 0,
        completionTokens: data.usage.completion_tokens || 0,
        totalTokens: data.usage.total_tokens || 0
      } : undefined
    };
    
  } catch (error) {
    if (error instanceof Error && error.name === 'AbortError') {
      throw new Error(`è¯·æ±‚è¶…æ—¶ (${config.timeout}ms)`);
    }
    throw error;
  }
}

/**
 * åŸå§‹AIå“åº”å®ç°ï¼ˆå›é€€ç”¨ï¼‰
 */
async function generateOriginalAIResponse(
  character: AICharacter,
  messages: Message[],
  estimateTokensFn?: (text: string) => number,
  updateTokenUsageFn?: (characterId: string, characterName: string, inputTokens: number, outputTokens: number, type: string) => void
): Promise<{
  response: string | null;
  responseTime: number;
  tokenUsage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}> {
  const startTime = Date.now();
  
  const config = {
    baseUrl: character.modelConfig?.baseUrl || modelDefaults.baseUrl,
    apiKey: character.modelConfig?.apiKey || modelDefaults.apiKey,
    modelName: character.modelConfig?.modelName || modelDefaults.modelName,
    prompt: character.modelConfig?.prompt || '',
    temperature: character.modelConfig?.temperature ?? 0.7,
    maxTokens: character.modelConfig?.maxTokens ?? 2048,
    topP: character.modelConfig?.topP ?? 1.0,
    frequencyPenalty: character.modelConfig?.frequencyPenalty ?? 0.0,
    presencePenalty: character.modelConfig?.presencePenalty ?? 0.0,
  };
  
  if (!config.baseUrl || !config.apiKey || !config.modelName) {
    throw new Error('æ¨¡å‹é…ç½®ä¸å®Œæ•´');
  }
  
  const requestMessages = [
    ...(config.prompt ? [{ role: 'system', content: config.prompt }] : []),
    ...messages.map(m => ({ 
      role: m.isPlayer ? 'user' : 'assistant', 
      content: m.text || m.content || '' 
    }))
  ];
  
  const requestBody = {
    model: config.modelName,
    messages: requestMessages,
    temperature: config.temperature,
    max_tokens: config.maxTokens,
    top_p: config.topP,
    frequency_penalty: config.frequencyPenalty,
    presence_penalty: config.presencePenalty,
  };

  const res = await fetch(config.baseUrl + '/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`,
    },
    body: JSON.stringify(requestBody),
  });
  
  if (!res.ok) {
    throw new Error(`APIè¯·æ±‚å¤±è´¥: ${res.status}`);
  }
  
  const data = await res.json();
  const responseContent = data.choices?.[0]?.message?.content || null;
  
  const responseTime = Date.now() - startTime;
  
  // è®°å½•Tokenä½¿ç”¨
  let tokenUsage;
  if (responseContent && updateTokenUsageFn && estimateTokensFn) {
    let inputTokens = data.usage?.prompt_tokens;
    let outputTokens = data.usage?.completion_tokens;
    
    if (!inputTokens || !outputTokens) {
      const inputText = requestMessages.map(m => m.content).join(' ');
      inputTokens = estimateTokensFn(inputText);
      outputTokens = estimateTokensFn(responseContent);
    }
    
    updateTokenUsageFn(character.id, character.name, inputTokens, outputTokens, 'fallback-character');
    
    tokenUsage = {
      promptTokens: inputTokens,
      completionTokens: outputTokens,
      totalTokens: inputTokens + outputTokens
    };
  }
  
  return {
    response: responseContent,
    responseTime,
    tokenUsage
  };
}

/**
 * åˆå§‹åŒ–å¢å¼ºAIå“åº”ç³»ç»Ÿ
 */
export function initializeEnhancedAIResponse(config?: Partial<EnhancedAIResponseConfig>): void {
  if (config) {
    Object.assign(defaultConfig, config);
  }
  
  console.log('ğŸš€ å¢å¼ºAIå“åº”ç³»ç»Ÿå·²åˆå§‹åŒ–');
  console.log('âš™ï¸ é…ç½®:', {
    å¯ç”¨ä¸Šä¸‹æ–‡è£å‰ª: defaultConfig.enableContextPruning,
    æœ€å¤§ä¸Šä¸‹æ–‡Token: defaultConfig.maxContextTokens,
    å¯ç”¨ä¸ªæ€§åŒ–: defaultConfig.enablePersonalization,
    è°ƒè¯•æ¨¡å¼: defaultConfig.debugMode
  });
}

/**
 * æ›´æ–°å¢å¼ºAIå“åº”é…ç½®
 */
export function updateEnhancedAIResponseConfig(config: Partial<EnhancedAIResponseConfig>): void {
  Object.assign(defaultConfig, config);
  console.log('ğŸ”§ å¢å¼ºAIå“åº”é…ç½®å·²æ›´æ–°');
}

/**
 * è·å–å¢å¼ºAIå“åº”ç»Ÿè®¡ä¿¡æ¯
 */
export function getEnhancedAIResponseStats() {
  return {
    contextManager: contextManager.getPerformanceStats(),
    processingHistory: contextManager.getProcessingHistory(20)
  };
}

/**
 * æ¸…ç†å¢å¼ºAIå“åº”ç¼“å­˜
 */
export function clearEnhancedAIResponseCache(): void {
  contextManager.clearCache();
  console.log('ğŸ§¹ å¢å¼ºAIå“åº”ç¼“å­˜å·²æ¸…ç†');
}

// å¯¼å‡ºä¾¿æ·å‡½æ•°
export { contextManager, processContextForAI }; 